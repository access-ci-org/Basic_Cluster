---
#OpenHPC release version
  openhpc_release_rpm: "http://repos.openhpc.community/OpenHPC/2/CentOS_8/x86_64/ohpc-release-2-1.el8.x86_64.rpm"
#The full list of available releases can be viewed at
# https://github.com/openhpc/ohpc/releases/ 
#
# Headnode Info
  public_interface: "enp0s3" # NIC that allows access to the public internet
  private_interface: "enp0s8" #NIC that allows access to compute nodes
  headnode_private_ip: "10.0.0.1"
  build_kernel_ver: '4.18.0-193.6.3.el8_2.x86_64' # `uname -r` at build time... for wwbootstrap
  dhcpd_enabled: true

#Open OnDemand Variables
  public_servername: "swosu-duke.offn.onenet.net"
  ssl_cert_key_file_path: "/etc/pki/tls/private/swosu-duke.offn.onenet.net/key.pem"
  ssl_cert_file_path: "/etc/pki/tls/certs/swosu-duke.offn.onenet.net/cert.pem"
  ssl_chain_file_path: ""
  headnode_local_homedirs: true # true unless mounting user homedirs from NFS
  ood_install: true #Turn OOD install on or off
  public_ip: "156.110.41.132" #Publicly accessible IP for httpd24-httpd

#Private network Info
  private_network: "10.0.0.0"  
  private_network_mask: "24"
  private_network_long_netmask: "255.255.255.0"
  compute_ip_minimum: "10.0.0.2"
  compute_ip_maximum: "10.0.0.255"
  gpu_ip_minimum: "10.0.0.128" #This could be more clever, like compute_ip_minimum + num_nodes...
  enable_compute_NAT: true

#slurm.conf variables
  cluster_name: "xcbc-example"
#  gres_types: "gpu"

#Stateful compute or not?
  stateful_nodes: false

#Node Config Vars - for stateful nodes
  sda1: "mountpoint=/boot:dev=sda1:type=ext3:size=500"
  sda2: "dev=sda2:type=swap:size=500"
  sda3: "mountpoint=/:dev=sda3:type=ext3:size=fill"

# GPU Node Vars
# download the nvidia cuda installer, and run with only --extract=$path_to_CRI_XCBC/roles/gpu_build_vnfs/files to get these three installers
  nvidia_driver_installer: "NVIDIA-Linux-x86_64-387.26.run"
  cuda_toolkit_installer: "cuda-linux.9.1.85-23083092.run"
  cuda_samples_installer: "cuda-samples.9.1.85-23083092-linux.run"


# WW Template Names for wwmkchroot
  template_path: "/usr/libexec/warewulf/wwmkchroot/"
  compute_template: "rocky-8"
  gpu_template: "gpu-nodes"  
  login_template: "login-nodes"  

# Chroot variables
  compute_chroot_loc: "/opt/ohpc/admin/images/{{ compute_chroot }}"
  compute_chroot: rocky8-compute
  gpu_chroot_loc: "/opt/ohpc/admin/images/{{ gpu_chroot }}"
  gpu_chroot: rocky8-gpu
  login_chroot_loc: "/opt/ohpc/admin/images/{{ login_chroot }}"
  login_chroot: rocky8-login 

# Node Inventory method - true: automatic, or false: manual
# Setting this to 'false' uses the list provided in the compute_nodes list below to add
# nodes to the Warewulf databse. 'true' uses the wwnodescan utility to automatically
# detect and add nodes.
  node_inventory_auto: true

#Node naming variables - no need to change
  compute_node_prefix: "compute-"
  num_compute_nodes: 2
  gpu_node_prefix: "gpu-compute-"
  num_gpu_nodes: 1
  login_node_prefix: "login-"
  num_login_nodes: 0

#Node Inventory - not in the Ansible inventory sense! Just for WW and Slurm config.
# Someday I will need a role that can run wwnodescan, and add nodes to this file! Probably horrifying practice.
# There is a real difference between building from scratch, and using these for maintenance / node addition!
#
  compute_private_nic: "eth0"
  compute_nodes:
   - { name: "compute-1", vnfs: '{{compute_chroot}}',  cpus: 1, sockets: 1, corespersocket: 1,  mac: "08:00:27:EC:E2:FF", ip: "10.0.0.254"}

  login_nodes:
   - { name: "login-1", vnfs: '{{login_chroot}}', cpus: 8, sockets: 2, corespersocket: 4,  mac: "00:26:b9:2e:21:ed", ip: "10.2.255.137"}
 
  gpu_nodes:
   - { name: "gpu-compute-1", vnfs: '{{gpu_chroot}}', gpus: 4, gpu_type: "gtx_TitanX", cpus: 16, sockets: 2, corespersocket: 8,  mac: "0c:c4:7a:6e:9d:6e", ip: "10.2.255.47"}
 
  viz_nodes:
   - { name: "viz-node-0-0", vnfs: '{{ gpu_chroot }}', gpus: 2, gpu_type: nvidia_gtx_780, cpus: 8, sockets: 2, corespersocket: 4,  mac: "foo", ip: "bar"}

#Slurm Accounting Variables - little need to change these
  slurm_acct_db: "slurmdb"
  slurmdb_storage_port: "7031"
  slurmdb_port: "1234"
  slurmdb_sql_pass: "password" #could force this to be a hash... 
  slurmdb_sql_user: slurm

# Active Directory Integration
  enable_active_directory: false

#automatic variables for internal use
# Don't edit these!
  compute_node_glob: "{{ compute_node_prefix }}[0-{{ num_compute_nodes|int - 1}}]"
  gpu_node_glob: "{{ gpu_node_prefix }}[0-{{ num_gpu_nodes|int - 1}}]"
  node_glob_bash: "{{ compute_node_prefix }}{0..{{ num_compute_nodes|int - 1}}}"
  gpu_node_glob_bash: "{{ compute_node_prefix }}{0..{{ num_compute_nodes|int - 1}}}"
