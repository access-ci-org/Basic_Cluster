---
#OpenHPC release version
  openhpc_release_rpm: "https://github.com/openhpc/ohpc/releases/download/v1.3.GA/ohpc-release-1.3-1.el7.x86_64.rpm"
#The full list of available versions for CentOS can be generated via
# curl -s https://github.com/openhpc/ohpc/releases/ | grep rpm | grep -v sle | grep -v strong  | sed 's/.*="\(.*\)".*".*".*/\1/'
#
# Headnode Info
  public_interface: "eth0" # NIC that allows access to the public internet
  private_interface: "eth1" #NIC that allows access to compute nodes
  headnode_private_ip: "10.1.1.10"
  build_kernel_ver: '3.10.0-957.12.2.el7.x86_64' # `uname -r` at build time... for wwbootstrap

#Private network Info
  private_network: "10.1.1.0"
  private_network_mask: "24"
  private_network_long_netmask: "255.255.255.0"
  compute_ip_minimum: "10.1.1.2"
  compute_ip_maximum: "10.1.1.255"
  gpu_ip_minimum: "10.1.1.128" #This could be more clever, like compute_ip_minimum + num_nodes...

#slurm.conf variables
  cluster_name: "CoD"
  cluster_title: "CoD Cluster"
  cluster_host: "login001"
  slurm_bin_path: "/cm/shared/apps/slurm/current/bin"
  slurm_conf_path: "/cm/shared/apps/slurm/var/etc/slurm/slurm.conf"
#  gres_types: "gpu"

# sacct user list
  cluster_users:
    - centos   # include each username on separate line as a list

#Stateful compute or not?
  stateful_nodes: false

#Node Config Vars - for stateful nodes
  sda1: "mountpoint=/boot:dev=sda1:type=ext3:size=500"
  sda2: "dev=sda2:type=swap:size=500"
  sda3: "mountpoint=/:dev=sda3:type=ext3:size=fill"

# GPU Node Vars
# download the nvidia cuda installer, and run with only --extract=$path_to_CRI_XCBC/roles/gpu_build_vnfs/files to get these three installers
  nvidia_driver_installer: "NVIDIA-Linux-x86_64-387.26.run"
  cuda_toolkit_installer: "cuda-linux.9.1.85-23083092.run"
  cuda_samples_installer: "cuda-samples.9.1.85-23083092-linux.run"


# WW Template Names for wwmkchroot
  template_path: "/usr/libexec/warewulf/wwmkchroot/"
  compute_template: "compute-nodes"
  gpu_template: "gpu-nodes"
  login_template: "login-nodes"

# Chroot variables
  compute_chroot_loc: "/opt/ohpc/admin/images/{{ compute_chroot }}"
  compute_chroot: centos7-compute
  gpu_chroot_loc: "/opt/ohpc/admin/images/{{ gpu_chroot }}"
  gpu_chroot: centos7-gpu
  login_chroot_loc: "/opt/ohpc/admin/images/{{ login_chroot }}"
  login_chroot: centos7-login
  bright_cod_login_chroot_loc: "/cm/images/{{ bright_cod_login_chroot }}"
  bright_cod_login_chroot: login-image
  bright_cod_compute_chroot_loc: "/cm/images/{{ bright_cod_compute_chroot }}"
  bright_cod_compute_chroot: compute-image

# Node Inventory method - automatic, or manual
  node_inventory_auto: false

#Node naming variables - no need to change
  compute_node_prefix: "c"
  compute_node_format: "{{ compute_node_prefix }}%04d"
  num_compute_nodes: 1
  gpu_node_prefix: "gpu-compute-"
  num_gpu_nodes: 1
  login_node_prefix: "login"
  login_node_format: "{{ login_node_prefix }}%03d"
  num_login_nodes: 0

#OpenOnDemand
  ood_nodename: "ood"
  ood_version: 2.0
  ood_private_ip: "10.1.1.11"
  ood_rpm_repo: "https://yum.osc.edu/ondemand/{{ ood_version }}/ondemand-release-web-{{ ood_version }}-1.noarch.rpm"
  ood_sys_apps_dir: "/var/www/ood/apps/sys"
  # a regex to extract username from remote_user in user_auth script"
  ood_user_regex: "^(.+)$"

#Node Inventory - not in the Ansible inventory sense! Just for WW and Slurm config.
# Someday I will need a role that can run wwnodescan, and add nodes to this file! Probably horrifying practice.
# There is a real difference between building from scratch, and using these for maintenance / node addition!
#
  compute_private_nic: "eth0"
  compute_nodes:
   - { name: "compute-1", vnfs: '{{ compute_chroot }}',  cpus: 1, sockets: 1, corespersocket: 1,  mac: "08:00:27:EC:E2:FF", ip: "10.0.0.254"}

  login_nodes:
   - { name: "login-1", vnfs: '{{ login_chroot }}', cpus: 8, sockets: 2, corespersocket: 4,  mac: "00:26:b9:2e:21:ed", ip: "10.2.255.137"}

  gpu_nodes:
   - { name: "gpu-compute-1", vnfs: '{{ gpu_chroot }}', gpus: 4, gpu_type: "gtx_TitanX", cpus: 16, sockets: 2, corespersocket: 8,  mac: "0c:c4:7a:6e:9d:6e", ip: "10.2.255.47"}

  viz_nodes:
   - { name: "viz-node-0-0", vnfs: gpu_chroot, gpus: 2, gpu_type: nvidia_gtx_780, cpus: 8, sockets: 2, corespersocket: 4,  mac: "foo", ip: "bar"}

#Slurm Accounting Variables - little need to change these
  slurm_acct_db: "slurmdb"
  slurmdb_storage_port: "7031"
  slurmdb_port: "1234"
  slurmdb_sql_pass: "password" #could force this to be a hash...
  slurmdb_sql_user: slurm

#automatic variables for internal use
# Don't edit these!
  compute_node_glob: "{{ compute_node_prefix }}[0-{{ num_compute_nodes|int - 1 }}]"
  gpu_node_glob: "{{ gpu_node_prefix }}[0-{{ num_gpu_nodes|int - 1 }}]"
  node_glob_bash: "{{ compute_node_prefix }}{0..{{ num_compute_nodes|int - 1 }}}"
  gpu_node_glob_bash: "{{ compute_node_prefix }}{0..{{ num_compute_nodes|int - 1 }}}"

#Jupyter related
  jupyter_provision: false
  jupyter_ood_app_repo: "https://github.com/OSC/bc_example_jupyter.git"
  jupyter_ood_app_refspec: "{{ github_refspec }}"
  jupyter_ood_app_version: "custom_environment"

#EasyBuild variables
  cluster_shared_folder: "/export"
  easybuild_prefix: "{{ cluster_shared_folder }}/eb"
  easybuild_tmpdir: "/tmp"
  easybuild_buildpath: "/tmp/build"
  easybuild_sourcepath: "/tmp/source"

#matlab install related
  matlab_provision: false
  matlab_download_url: "https://uab.box.com/shared/static/y01qu7oo1gpne6j2s6nqwcuee63epivo.gz"
  matlab_clustershare: "/opt/ohpc/pub/apps/matlab/"
  matlab_destination: "/tmp/matlab.tar.gz"
  # module file vars
  matlab_install_root: "/opt/ohpc/pub-master/apps/matlab/M2/"
  matlab_docs_url: "http://{{ ood_nodename }}"
  matlab_license_file: "{{ matlab_install_root }}/licenses/licenses.lic"
  matlab_module_path: "{{ easybuild_prefix }}/modules/all"
  matlab_module_appdir: "matlab"
  matlab_module_file: "r2018a"
  matlab_ver: "{{ matlab_module_file }}"
  matlab_ood_app_repo: "https://github.com/OSC/bc_osc_matlab.git"
  matlab_ood_app_refspec: "{{ github_refspec }}"
  matlab_ood_app_version: "master"

#SAS install related
  sas_provision: false
  sas_clustershare: "/export/apps/sas/"
  sas_module_path: "{{ easybuild_prefix }}/modules/all"
  sas_module_appdir: "sas"
  sas_module_file: "9.4"
  sas_ver: "{{ sas_module_file }}"
  sas_ood_app_repo: "https://github.com/uabrc/ood_sas.git"
  sas_ood_app_refspec: "{{ github_refspec }}"
  sas_ood_app_version: "master"

#ANSYS install related
  ansys_provision: false
  ansys_module_name: "ANSYS"
  ansys_install_root: "{{ cluster_shared_folder }}/apps/{{ ansys_module_name }}"
  ansys_modulefiles_dir: "{{ cluster_shared_folder }}/modulefiles/{{ ansys_module_name }}"
  ansys_ood_app_dir: "{{ ood_sys_apps_dir }}/{{ ansys_module_name }}"
  ansys_ood_repo: "http://github.com/uabrc/ood_ansys.git"
  ansys_versions:
    - "19.1"

#IGV install related
  igv_provision: false
  igv_module_name: "IGV"
  igv_install_root: "{{ cluster_shared_folder }}/apps/{{ igv_module_name }}"
  igv_modulefiles_dir: "{{ cluster_shared_folder }}/modulefiles/{{ igv_module_name }}"
  igv_ood_app_dir: "{{ ood_sys_apps_dir }}/{{ igv_module_name }}"
  igv_ood_repo: "https://github.com/uabrc/ood_igv.git"
  igv_versions:
    - "2.5.0-Java-11"

#Rstudio related
  rstudio_ood_app_repo: "https://github.com/rtripath89/bc_osc_rstudio_server.git"
  rstudio_ood_app_refspec: "{{ github_refspec }}"
  rstudio_ood_app_version: "0.6.0"
  rstudio_provision: false
  singularity_ver: '2.4.2'
  singularity_rstudio_app_repo: "https://github.com/nickjer/singularity-rstudio.git"
  singularity_rstudio_app_refspec: "{{ github_refspec }}"
  singularity_rstudio_app_version: "{{ master }}"
  r_versions:
    - { full: '3.5.1', short: '3.5' }
    - { full: '3.4.4', short: '3.4' }

#Copr Repos
  enable_copr: true
  copr_repos:
    - { repo_name: "louistw/mod_wsgi-3.4-18-httpd24", host: ["{{ ood_nodename }}"] }
    - { repo_name: "louistw/slurm-17.11.11-ohpc-1.3.6", host: ["{{ cluster_name }}", "{{ ood_nodename }}"] }
    - { repo_name: "atlurie/shibboleth-3.0-ood", host: ["{{ ood_nodename }}"] }

# Shibboleth SSO
  install_shib: false
  configure_shib: false

# LTS-S3 config
  lts_access_key:  ACCESS_KEY
  lts_secret_key: SECRET_KEY
  s3_endpoint:  S3_ENDPOINT

#S3-shib_config
  s3_shibboleth_bucket_name: UAB_SHIB_BUCKET
  s3_shibboleth_object_name: UAB_SHIB_OBJECT

# User Registration
  enable_user_reg: true
  user_register_app: "flask_user_reg"
  user_register_app_tag: "5c06809cf494db0f268c325fbc309a40eb5a6eea"
  user_register_app_path: "/var/www/ood/register/{{ user_register_app }}"
  user_register_app_key: "1234"
  user_register_app_repo: "https://gitlab.rc.uab.edu/rc/self-reg-form.git"
  user_register_app_refspec: "{{ gitlab_refspec }}"
  user_register_app_port: 8000
  cors_allowed_origins: "*"
  mod_wsgi_pkg_name: "uab-httpd24-mod_wsgi"
  RegUser_app_user: "reggie"
  RegUser_app_user_full_name: "RegUser of user register app"
  RegUser_app_user_passwd: "qweasd"
# Authorized user group
  valid_eppa:  ["faculty", "staff", "student", "affiliate"]

# RabbitMQ
  rabbitmq_provision: true
  rabbitmq_user: "reggie"
  rabbitmq_user_password: "reggie"
  rabbitmq_host: "master"
  rabbitmq_exchange: "RegUsr"
  rabbitmq_vhost: "adduser"
  rabbitmq_port: "5672"
  rabbitmq_agents_loc: "/cm/shared/rabbitmq_agents"
  rabbitmq_agents_repo: "https://github.com/uabrc/rabbitmq_agents.git"
  rabbitmq_agents_version: "feat-cod-rmq"
  rabbitmq_agents_refspec: "{{ github_refspec }}"
  rabbitmq_agents_service_user: "root"
  rabbitmq_agents_db_path: ".agent_db"

  # rabbit_config
  mail_server: "localhost"
  admin_email: "root@localhost"
  sender: "ROOT@LOCALHOST"
  sender_alias: "Services"
  subject: "New User Account"
  info_url: "https://www.google.com"
  mail_list: "root@localhost"
  mail_list_bcc: "cmsupport@localhost"
  function_timeout: 30

  delay: "5" #time delay to let account creation finish to avoid concurrency
  user_dirs: ['/home', '/data/user', '/data/scratch']
  rc_users_ldap_repo: "git@gitlab.rc.uab.edu:atlurie/rc-user-add-test.git"
  rc_users_ldap_repo_loc: "~/git/rc-users"
  valid_user_state: ['ok', 'blocked', 'certification']

# Celery
  celery_user: "reggie"
  celery_user_password: "reggie"
  celery_vhost: "socketio"

#ClusteronDemand CoD
  cod_deploy: true
  post_create_script: PostAddUserScript.sh

# Lmod
  lmod_loc: "/opt/ohpc/admin/lmod/lmod"
  lmod_archive_loc: "{{ cluster_shared_folder }}/rc/lmod"
  lmod_log_loc: "/var/log"
  lmod_log_filename: "{{ lmod_log_loc }}/moduleUsage.log"
  lmod_module_tracking_loc: "{{ lmod_loc }}/tools/tracking_module_usage"
  lmod_repo: "https://github.com/TACC/Lmod.git"
  lmod_refspec: "{{ github_refspec }}"
  lmod_version: "8.5.9"
  lmod_cron_user: "root"
  lmod_share_loc: "{{ cluster_shared_folder }}/lmod_script"
  lmod_db: "lmod"
  lmod_db_user: "lmod"
  lmod_db_passwd: "password"
  lmod_db_allow_host: "localhost"
  lmod_db_host_machine: "ohpc"

# refspec for merge/pull request
# with refspec set up, you can specify version as:
# gitlab: mr/MR_ID
# github: pr/PR_ID
  gitlab_refspec: "+refs/merge-requests/*/head:refs/remotes/origin/mr/*"
  github_refspec: "+refs/pull/*/head:refs/remotes/origin/pr/*"
